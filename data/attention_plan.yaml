title: Attention From Scratch
subtitle: 12-week plan to build a from-scratch Transformer inference stack (prefill, decode, serving)
start_date: 2025-10-14
weeks:
  - id: week-1
    week: 1
    title: Vast.AI Setup + Baseline Environment
    target_date: '2025-10-21'
    completed: false
    completed_date: ''
    summary: Provision a GPU instance, set up toolchains and libraries, and capture a baseline with a reference LLM.
    details: >-
      Create a repeatable Vast.AI environment. Install build tools, CUDA/CUTLASS, SentencePiece, and HF libs.
      Run a reference model (Mistral-7B or Llama-3-8B) to capture tokens/s, latency, VRAM usage — your baseline.
    checklist:
      - Create Vast.AI account and allocate A100/4090 GPU
      - Create a persistent Volume for models and build cache
      - Launch image: nvcr.io/nvidia/pytorch:25.04-py3
      - Install build tools (git, cmake, ninja, g++, clang, python3-dev)
      - Install libs (torch, transformers, safetensors, sentencepiece, numpy, cutlass, pybind11)
      - Verify CUDA/NVCC and run a sample kernel
      - Run reference LLM and log: tokens/s, VRAM, prefill vs decode latency
      - Save results as baseline report

  - id: week-2
    week: 2
    title: Repo Scaffolding + Tokenizer + Weight Loader
    target_date: '2025-10-28'
    completed: false
    completed_date: ''
    summary: Scaffold the project and implement tokenizer wrapper and safetensors weight loader with tests.
    details: >-
      Create C++/CUDA skeleton with CMake and pybind11 bindings. Implement SentencePiece wrapper and
      a safetensors loader with shape/name validation. Add unit tests with small diffs.
    checklist:
      - Init src/, include/, python_bindings/, tests/, scripts/, benchmarks/
      - CMakeLists for shared lib and exe targets
      - SentencePiece tokenizer wrapper (+ round-trip tests)
      - Safetensors reader to row-major fp16; validate names/shapes
      - Unit tests: mean abs diff < 1e-5 on sampled tensors

  - id: week-3
    week: 3
    title: Forward Pass Framework + Basic Kernels
    target_date: '2025-11-04'
    completed: false
    completed_date: ''
    summary: Build the forward graph and write core kernels for RMSNorm, RoPE, matmul, MLP, and naïve attention.
    details: >-
      Implement RMSNorm and RoPE kernels. Use cuBLAS/CUTLASS for matmul wrappers. Implement SwiGLU MLP and
      a simple attention path. Compare layerwise activations against HF.
    checklist:
      - Define ModelConfig (layers, heads, dims, rope_theta)
      - RMSNorm kernel validated vs PyTorch
      - RoPE kernel
      - Matmul wrapper (cuBLAS/CUTLASS)
      - MLP (SwiGLU)
      - Naïve QKV attention + softmax
      - Prefill-only forward; layerwise MAE < 1e-3

  - id: week-4
    week: 4
    title: Prefill + Decode Loop + Sampling
    target_date: '2025-11-11'
    completed: false
    completed_date: ''
    summary: Implement KV cache, decode loop, and sampling strategies; validate generations and measure performance.
    details: >-
      Add contiguous KV cache, greedy/top-k/top-p/temperature sampling, and end-to-end decode. Measure throughput
      and latency; validate against HF on short prompts.
    checklist:
      - KV cache structure (batch, layer, seq, dim)
      - Prefill + token-by-token decode loop
      - Greedy, top-k, top-p, temperature
      - Validate text parity on short prompts
      - Record tokens/s, latency, memory use

  - id: week-5
    week: 5
    title: FlashAttention-Style Optimization
    target_date: '2025-11-18'
    completed: false
    completed_date: ''
    summary: Replace naïve attention with IO-aware tiled kernels; benchmark and profile.
    details: >-
      Implement shared-memory tiled softmax/attention; test on 128–4096 tokens. Benchmark vs week-4 and capture
      Nsight metrics (occupancy, memory bandwidth).
    checklist:
      - Review FlashAttention algorithm
      - Tiled softmax+attention kernel
      - Correctness on 128–4096 tokens
      - Benchmark vs week-4 attention
      - Nsight Compute profiling and timing macros

  - id: week-6
    week: 6
    title: Advanced KV Cache + CUDA Graphs
    target_date: '2025-11-25'
    completed: false
    completed_date: ''
    summary: Introduce paged/slab KV cache and capture the decode path with CUDA Graphs.
    details: >-
      Design a paged allocator (vLLM-style) handling variable seq lengths and in-flight batching. Add CUDA Graphs
      for the decode chain and micro-bench scheduling overhead.
    checklist:
      - Paged/slab KV allocator (vLLM-inspired)
      - Variable seq lengths + in-flight batching
      - CUDA Graph capture for decode chain
      - Micro-bench capture overhead; target ≤ 1 µs per token

  - id: week-7
    week: 7
    title: Quantization (AWQ / GPTQ / NF4)
    target_date: '2025-12-02'
    completed: false
    completed_date: ''
    summary: Implement weight-only int4 paths and evaluate quality vs fp16 baseline.
    details: >-
      Implement group-wise quantization with on-the-fly dequant in GEMM, compare AWQ/GPTQ/NF4 behavior and regressions.
    checklist:
      - Group-wise quantization (e.g., 128-group)
      - Scales/zeros and storage format
      - GEMM dequant on-the-fly
      - Logit diff within tolerance vs fp16
      - Throughput & memory evaluation

  - id: week-8
    week: 8
    title: Batching, Streams, and Scheduler
    target_date: '2025-12-09'
    completed: false
    completed_date: ''
    summary: Improve throughput via concurrent streams and request scheduling.
    details: >-
      Build prefill/decode queues with separate CUDA streams, overlap tokenization and H2D copies, and benchmark
      mixed prompt lengths.
    checklist:
      - Request queues (prefill/decode)
      - Separate CUDA streams for prefill/copy
      - Overlap tokenization and H2D
      - ≥2× tokens/s vs week-6 with batching

  - id: week-9
    week: 9
    title: Parallelism & Memory Optimization
    target_date: '2025-12-16'
    completed: false
    completed_date: ''
    summary: Extend to multi-GPU tensor parallel and reduce memory pressure for long contexts.
    details: >-
      Shard QKV/MLP across GPUs with NCCL all-reduce; add activation checkpointing for long contexts and measure scaling.
    checklist:
      - Tensor parallel sharding for QKV/MLP
      - NCCL all-reduce wiring and tests
      - 1→2 GPU scaling measurements
      - Activation checkpointing for long contexts

  - id: week-10
    week: 10
    title: Speculative & Lookahead Decoding
    target_date: '2025-12-23'
    completed: false
    completed_date: ''
    summary: Implement Lookahead and (optionally) Medusa heads; measure speedups.
    details: >-
      Enable a lookahead path with rollback, add an optional Medusa multi-token predictor, and log speedup ratios on long outputs.
    checklist:
      - Lookahead decoding with rollback checks
      - Optional Medusa head adapter
      - Measure speedup vs standard decode

  - id: week-11
    week: 11
    title: Serving Layer
    target_date: '2025-12-30'
    completed: false
    completed_date: ''
    summary: Expose the engine via a lightweight API with batching and metrics.
    details: >-
      Implement HTTP endpoints (/v1/completions and /v1/chat/completions), add batching inside the server, metrics, and KV eviction.
    checklist:
      - FastAPI or C++ http server
      - /v1/completions and /v1/chat/completions
      - In-server batching
      - Metrics and LRU cache eviction
      - Concurrency test with ≥ 8 clients

  - id: week-12
    week: 12
    title: Validation, Docs, and Release
    target_date: '2026-01-06'
    completed: false
    completed_date: ''
    summary: Validate parity and publish a complete report and repo.
    details: >-
      Run numerical parity checks, benchmark against llama.cpp/vLLM/TensorRT-LLM, stress test long prompts, and publish
      a thorough write-up with results and lessons learned.
    checklist:
      - Logit diff < 1e-3 vs HF
      - Bench vs llama.cpp, vLLM, TensorRT-LLM
      - Long-prompt stress tests (> 8k tokens)
      - Technical report + diagrams + profiling
      - Public repo and (optional) demo video


