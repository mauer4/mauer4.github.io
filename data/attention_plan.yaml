title: Attention From Scratch
subtitle: 12-week plan to build a from-scratch Transformer inference stack (prefill, decode, serving)
start_date: 2025-10-14
weeks:
  - id: week-1
    week: 1
    title: Vast.AI Setup + Baseline Environment
    target_date: '2025-10-21'
    completed: true
    completed_date: '2025-10-19'
    summary: Baseline environment ready; captured reference runs on Olmo 2 (RTX 5090).
    details: >-
      Create a repeatable Vast.AI environment. Install build tools, CUDA/CUTLASS, SentencePiece, and HF libs.
      Run a reference model to capture tokens/s, latency, VRAM usage, and profiling artefacts for the baseline.
    checklist:
      - Create Vast.AI account and allocate A100/4090 GPU
      - Create a persistent Volume for models and build cache
      - Install build tools (git, cmake, ninja, g++, clang, python3-dev)
      - Install libs (torch, transformers, safetensors, sentencepiece, numpy, cutlass, pybind11)
      - Verify CUDA/NVCC and run a sample kernel
      - Run baseline generation on GPU
    comments:
      - Got Olmo 2 running on an RTX 5090 with ~25.7 GiB VRAM in use and ~17 tokens/s throughput over 64 tokens.
      - Captured nsys profiling (check_olmo_gpu_profile.nsys-rep) and an nvidia-smi snapshot for the baseline report.

  - id: week-2
    week: 2
    title: Baseline Benchmark Harness
    target_date: '2025-10-28'
    completed: false
    completed_date: ''
    summary: Research performance metrics and implement a repeatable GPU benchmarking suite for reference models.
    details: >-
      Investigate industry metrics (MLPerf, service SLIs), define the measurements to track, and build scripts that
      execute baselines across GPUs and workloads with structured output for later comparison.
    checklist:
      - Research benchmarking frameworks (MLPerf, Hugging Face evals, inference SLIs) and select metrics to track
      - Implement baseline runner capturing latency, throughput, and memory for configurable prompts and batch sizes
      - Validate harness locally on the 5090 and archive results alongside profiling reports
      - Plan GPU matrix and volume-testing scenarios for rented hardware
      - Set up logging/output format suitable for dashboards or reports

  - id: week-3
    week: 3
    title: Inference Stack Skeleton
    target_date: '2025-11-04'
    completed: false
    completed_date: ''
    summary: Start building the custom inference stack with modular orchestration and a prefill pipeline.
    details: >-
      Define boundaries between orchestrator, kernel modules, and telemetry. Stand up request queues, loader hooks,
      and a first-cut prefill path that reuses reference kernels while enabling swap-in of custom implementations later.
    checklist:
      - Define architecture diagram and module responsibilities (orchestrator, kernels, telemetry)
      - Implement loader plus request queue scaffolding with basic scheduling hooks
      - Wire the baseline runner into the stack for automated regression checks
      - Stand up the prefill path with existing kernels and capture end-to-end traces
      - Add unit/integration tests for the orchestration layer

  - id: week-4
    week: 4
    title: Prefill + Decode Loop + Sampling
    target_date: '2025-11-11'
    completed: false
    completed_date: ''
    summary: Implement KV cache, decode loop, and sampling strategies; validate generations and measure performance.
    details: >-
      Add contiguous KV cache, greedy/top-k/top-p/temperature sampling, and end-to-end decode. Measure throughput
      and latency; validate against HF on short prompts.
    checklist:
      - KV cache structure (batch, layer, seq, dim)
      - Prefill + token-by-token decode loop
      - Greedy, top-k, top-p, temperature
      - Validate text parity on short prompts
      - Record tokens/s, latency, memory use

  - id: week-5
    week: 5
    title: FlashAttention-Style Optimization
    target_date: '2025-11-18'
    completed: false
    completed_date: ''
    summary: Replace naive attention with IO-aware tiled kernels; benchmark and profile.
    details: >-
      Implement shared-memory tiled softmax/attention; test on 128-4096 tokens. Benchmark vs week-4 and capture
      Nsight metrics (occupancy, memory bandwidth).
    checklist:
      - Review FlashAttention algorithm
      - Tiled softmax+attention kernel
      - Correctness on 128-4096 tokens
      - Benchmark vs week-4 attention
      - Nsight Compute profiling and timing macros

  - id: week-6
    week: 6
    title: Advanced KV Cache + CUDA Graphs
    target_date: '2025-11-25'
    completed: false
    completed_date: ''
    summary: Introduce paged/slab KV cache and capture the decode path with CUDA Graphs.
    details: >-
      Design a paged allocator (vLLM-style) handling variable seq lengths and in-flight batching. Add CUDA Graphs
      for the decode chain and micro-bench scheduling overhead.
    checklist:
      - Paged/slab KV allocator (vLLM-inspired)
      - Variable seq lengths + in-flight batching
      - CUDA Graph capture for decode chain
      - Micro-bench capture overhead; target <= 1 microsecond per token

  - id: week-7
    week: 7
    title: Quantization (AWQ / GPTQ / NF4)
    target_date: '2025-12-02'
    completed: false
    completed_date: ''
    summary: Implement weight-only int4 paths and evaluate quality vs fp16 baseline.
    details: >-
      Implement group-wise quantization with on-the-fly dequant in GEMM, compare AWQ/GPTQ/NF4 behavior and regressions.
    checklist:
      - Group-wise quantization (e.g., 128-group)
      - Scales/zeros and storage format
      - GEMM dequant on-the-fly
      - Logit diff within tolerance vs fp16
      - Throughput & memory evaluation

  - id: week-8
    week: 8
    title: Batching, Streams, and Scheduler
    target_date: '2025-12-09'
    completed: false
    completed_date: ''
    summary: Improve throughput via concurrent streams and request scheduling.
    details: >-
      Build prefill/decode queues with separate CUDA streams, overlap tokenization and H2D copies, and benchmark
      mixed prompt lengths.
    checklist:
      - Request queues (prefill/decode)
      - Separate CUDA streams for prefill/copy
      - Overlap tokenization and H2D
      - >=2x tokens/s vs week-6 with batching

  - id: week-9
    week: 9
    title: Parallelism & Memory Optimization
    target_date: '2025-12-16'
    completed: false
    completed_date: ''
    summary: Extend to multi-GPU tensor parallel and reduce memory pressure for long contexts.
    details: >-
      Shard QKV/MLP across GPUs with NCCL all-reduce; add activation checkpointing for long contexts and measure scaling.
    checklist:
      - Tensor parallel sharding for QKV/MLP
      - NCCL all-reduce wiring and tests
      - 1->2 GPU scaling measurements
      - Activation checkpointing for long contexts

  - id: week-10
    week: 10
    title: Speculative & Lookahead Decoding
    target_date: '2025-12-23'
    completed: false
    completed_date: ''
    summary: Implement Lookahead and (optionally) Medusa heads; measure speedups.
    details: >-
      Enable a lookahead path with rollback, add an optional Medusa multi-token predictor, and log speedup ratios on long outputs.
    checklist:
      - Lookahead decoding with rollback checks
      - Optional Medusa head adapter
      - Measure speedup vs standard decode

  - id: week-11
    week: 11
    title: Serving Layer
    target_date: '2025-12-30'
    completed: false
    completed_date: ''
    summary: Expose the engine via a lightweight API with batching and metrics.
    details: >-
      Implement HTTP endpoints (/v1/completions and /v1/chat/completions), add batching inside the server, metrics, and KV eviction.
    checklist:
      - FastAPI or C++ http server
      - /v1/completions and /v1/chat/completions
      - In-server batching
      - Metrics and LRU cache eviction
      - Concurrency test with >= 8 clients

  - id: week-12
    week: 12
    title: Validation, Docs, and Release
    target_date: '2026-01-06'
    completed: false
    completed_date: ''
    summary: Validate parity and publish a complete report and repo.
    details: >-
      Run numerical parity checks, benchmark against llama.cpp/vLLM/TensorRT-LLM, stress test long prompts, and publish
      a thorough write-up with results and lessons learned.
    checklist:
      - Logit diff < 1e-3 vs HF
      - Bench vs llama.cpp, vLLM, TensorRT-LLM
      - Long-prompt stress tests (> 8k tokens)
      - Technical report + diagrams + profiling
      - Public repo and (optional) demo video
