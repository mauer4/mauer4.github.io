<!doctype html><html lang=en-us dir=ltr data-wc-theme-default=system><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Hugo Blox Builder 0.3.1"><meta name=author content="Adin Mauer"><meta name=description content="Hardware/RTL engineer focused on high-performance interconnects, parallel computing, and applied ML. Projects, posts, and notes."><link rel=alternate hreflang=en-us href=https://mauer4.github.io/attention-from-scratch/><link rel=stylesheet href=/css/themes/emerald.min.css><link href=/dist/wc.min.css rel=stylesheet><link href=/css/custom.min.0db33cb5559d47b61d5df219a67f5d40f03b4d8cf7ce2f531b0d8e2f6cecab8e.css rel=stylesheet><script>window.hbb={defaultTheme:document.documentElement.dataset.wcThemeDefault,setDarkTheme:()=>{document.documentElement.classList.add("dark"),document.documentElement.style.colorScheme="dark"},setLightTheme:()=>{document.documentElement.classList.remove("dark"),document.documentElement.style.colorScheme="light"}},console.debug(`Default Hugo Blox Builder theme is ${window.hbb.defaultTheme}`),"wc-color-theme"in localStorage?localStorage.getItem("wc-color-theme")==="dark"?window.hbb.setDarkTheme():window.hbb.setLightTheme():(window.hbb.defaultTheme==="dark"?window.hbb.setDarkTheme():window.hbb.setLightTheme(),window.hbb.defaultTheme==="system"&&(window.matchMedia("(prefers-color-scheme: dark)").matches?window.hbb.setDarkTheme():window.hbb.setLightTheme()))</script><script>document.addEventListener("DOMContentLoaded",function(){let e=document.querySelectorAll("li input[type='checkbox'][disabled]");e.forEach(e=>{e.parentElement.parentElement.classList.add("task-list")});const t=document.querySelectorAll(".task-list li");t.forEach(e=>{let t=Array.from(e.childNodes).filter(e=>e.nodeType===3&&e.textContent.trim().length>1);if(t.length>0){const n=document.createElement("label");t[0].after(n),n.appendChild(e.querySelector("input[type='checkbox']")),n.appendChild(t[0])}})})</script><link rel=icon type=image/png href=/media/icon_hu913444109597540128.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu1826128527410157824.png><link rel=canonical href=https://mauer4.github.io/attention-from-scratch/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="Adin Mauer Portfolio Website"><meta property="og:url" content="https://mauer4.github.io/attention-from-scratch/"><meta property="og:title" content="Attention From Scratch | Adin Mauer Portfolio Website"><meta property="og:description" content="Hardware/RTL engineer focused on high-performance interconnects, parallel computing, and applied ML. Projects, posts, and notes."><meta property="og:image" content="https://mauer4.github.io/media/icon_hu4060597685561840127.png"><meta property="twitter:image" content="https://mauer4.github.io/media/icon_hu4060597685561840127.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2025-01-01T00:00:00+00:00"><title>Attention From Scratch | Adin Mauer Portfolio Website</title><style>@font-face{font-family:inter var;font-style:normal;font-weight:100 900;font-display:swap;src:url(/dist/font/Inter.var.woff2)format(woff2)}</style><link type=text/css rel=stylesheet href=/dist/lib/katex/katex.min.505d5f829022bb7b4f24dfee0aa1141cd7bba67afe411d1240335f820960b5c3.css integrity="sha256-UF1fgpAiu3tPJN/uCqEUHNe7pnr+QR0SQDNfgglgtcM="><script defer src=/dist/lib/katex/katex.min.dc84b296ec3e884de093158f760fd9d45b6c7abe58b5381557f4e138f46a58ae.js integrity="sha256-3ISyluw+iE3gkxWPdg/Z1Ftser5YtTgVV/ThOPRqWK4="></script><script defer src=/js/katex-renderer.6579ec9683211cfb952064aedf3a3baea5eeb17a061775b32b70917474637c80.js integrity="sha256-ZXnsloMhHPuVIGSu3zo7rqXusXoGF3WzK3CRdHRjfIA="></script><script defer src=/js/hugo-blox-en.min.8c8ea06bd0420f5067e52fa727b9f92303757322ba4431774153d59a9735eadb.js integrity="sha256-jI6ga9BCD1Bn5S+nJ7n5IwN1cyK6RDF3QVPVmpc16ts="></script><script async defer src=https://buttons.github.io/buttons.js></script></head><body class="dark:bg-hb-dark dark:text-white page-wrapper" id=top><div id=page-bg></div><div class="page-header sticky top-0 z-30"><header id=site-header class=header><nav class="navbar px-3 flex justify-left"><div class="order-0 h-100"><a class=navbar-brand href=/ title="Adin Mauer Portfolio Website">Adin Mauer</a></div><input id=nav-toggle type=checkbox class=hidden>
<label for=nav-toggle class="order-3 cursor-pointer flex items-center lg:hidden text-dark dark:text-white lg:order-1"><svg id="show-button" class="h-6 fill-current block" viewBox="0 0 20 20"><title>Open Menu</title><path d="M0 3h20v2H0V3zm0 6h20v2H0V9zm0 6h20v2H0V0z"/></svg><svg id="hide-button" class="h-6 fill-current hidden" viewBox="0 0 20 20"><title>Close Menu</title><polygon points="11 9 22 9 22 11 11 11 11 22 9 22 9 11 -2 11 -2 9 9 9 9 -2 11 -2" transform="rotate(45 10 10)"/></svg></label><ul id=nav-menu class="navbar-nav order-3 hidden lg:flex w-full pb-6 lg:order-1 lg:w-auto lg:space-x-2 lg:pb-0 xl:space-x-8 justify-left"><li class=nav-item><a class=nav-link href=/>Bio</a></li><li class=nav-item><a class=nav-link href=/experience/>Experience</a></li><li class=nav-item><a class=nav-link href=/projects/>Projects</a></li><li class=nav-item><a class=nav-link href=/post/>Blog</a></li><li class=nav-item><a class="nav-link active" href=/attention-from-scratch/>Attention From Scratch</a></li></ul><div class="order-1 ml-auto flex items-center md:order-2 lg:ml-0"><div class="px-3 text-black hover:text-primary-700 dark:text-white dark:hover:text-primary-300
[&.active]:font-bold [&.active]:text-black/90 dark:[&.active]:text-white"><button class="theme-toggle mt-1" accesskey=t title=appearance><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="dark:hidden"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="dark:block [&:not(dark)]:hidden"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div></nav></header></div><div class="page-body my-10"><div class="max-w-prose mx-auto flex justify-center"><article class="prose prose-slate lg:prose-xl dark:prose-invert"><h1 class=lg:text-6xl>Attention From Scratch</h1><p>I would like to build a project to code AI inference transformers from scratch for an open source LLM — that is, use open-source LLM weights and implement all inference code and kernels top-to-bottom, including prefill, decode, and serving. I plan to develop and run this on rented NVIDIA server GPUs via Vast.ai. This is a realistic, first‑principles path toward learning how to implement and operate a production‑level LLM inference stack on cloud, enterprise‑grade CPU/GPU hardware.</p><section class=attention-hero><div class=container><h1>Attention From Scratch</h1><p class=subtitle>12-week plan to build a from-scratch Transformer inference stack (prefill, decode, serving)</p><div class=progress><div class=progress-label>Progress: 0/12 (0%)</div><div class=progress-outer aria-label="Plan progress" role=progressbar aria-valuemin=0 aria-valuemax=100 aria-valuenow=0><div class=progress-inner style=width:0%></div></div></div></div></section><section class=attention-timeline><div class=timeline><div class=timeline-item data-id=week-1 data-completed=false><div class=dot title=Planned></div><div class=content><div class=header><div class=left><div class=week>Week 1</div><div class=title><img class=icon src=/media/icons/timeline/transformer.svg alt="transformer icon">
<span>Loader, Tokenizer, Golden Harness (part 1)</span></div></div><div class=right></div></div><div class=summary>Implement safetensors weight loader and start SentencePiece tokenizer integration.</div><button class=toggle aria-expanded=false>Details</button><div class=details hidden><p>Implement a weight loader for Hugging Face safetensors and convert to a custom on-disk layout (row-major, per-channel scales). Verify tensor shapes vs model configs (Mistral-7B or Llama-3.1-8B). Plug in SentencePiece tokenizer and create byte-level parity tests vs HF tokenization.</p><div class=comments><div class=comments-title>Notes</div><ul><li>M1: layerwise diff harness by end of W2</li></ul></div></div></div></div><div class=timeline-item data-id=week-2 data-completed=false><div class=dot title=Planned></div><div class=content><div class=header><div class=left><div class=week>Week 2</div><div class=title><img class=icon src=/media/icons/timeline/transformer.svg alt="transformer icon">
<span>Loader, Tokenizer, Golden Harness (part 2)</span></div></div><div class=right></div></div><div class=summary>Build a correctness harness to diff logits layer-by-layer vs HF.</div><button class=toggle aria-expanded=false>Details</button><div class=details hidden><p>Run a reference forward in PyTorch/HF and diff logits/embeddings layer-by-layer within small tolerances. Solidify tokenizer bindings and I/O. Ensure the harness can run short prompts for fast iteration.</p><div class=comments><div class=comments-title>Notes</div><ul><li>Definition of done: ~1e-3 tol on a 64-token prompt</li></ul></div></div></div></div><div class=timeline-item data-id=week-3 data-completed=false><div class=dot title=Planned></div><div class=content><div class=header><div class=left><div class=week>Week 3</div><div class=title><img class=icon src=/media/icons/timeline/transformer.svg alt="transformer icon">
<span>Baseline Forward & Minimal Server (part 1)</span></div></div><div class=right></div></div><div class=summary>Implement core CUDA kernels and a basic C++ forward path.</div><button class=toggle aria-expanded=false>Details</button><div class=details hidden><p>Write kernels for RMSNorm, RoPE, QKV projection, a naïve attention, and MLP (SwiGLU). Assemble a clean C++ forward that consumes the loaded weights and tokenizer output.</p></div></div></div><div class=timeline-item data-id=week-4 data-completed=false><div class=dot title=Planned></div><div class=content><div class=header><div class=left><div class=week>Week 4</div><div class=title><img class=icon src=/media/icons/timeline/transformer.svg alt="transformer icon">
<span>Baseline Forward & Minimal Server (part 2)</span></div></div><div class=right></div></div><div class=summary>Add KV cache, sampling, and a minimal HTTP server.</div><button class=toggle aria-expanded=false>Details</button><div class=details hidden><p>Implement a contiguous per-layer KV cache (fp16/bf16). Add greedy/top-k/top-p sampling and temperature. Ship a minimal HTTP server with a basic completion route and request lifecycle.</p><div class=comments><div class=comments-title>Notes</div><ul><li>M2: ≥50 tok/s greedy on A100-40GB (single request)</li></ul></div></div></div></div><div class=timeline-item data-id=week-5 data-completed=false><div class=dot title=Planned></div><div class=content><div class=header><div class=left><div class=week>Week 5</div><div class=title><img class=icon src=/media/icons/timeline/attention.svg alt="attention icon">
<span>Flash-style Attention & KV Paged Cache (part 1)</span></div></div><div class=right></div></div><div class=summary>Replace naïve attention with FlashAttention-style kernels.</div><button class=toggle aria-expanded=false>Details</button><div class=details hidden><p>Implement I/O-aware attention kernels with tiling and numerically stable softmax. Keep code portable across A100/4090, with hooks for Hopper FA-3 features when available.</p></div></div></div><div class=timeline-item data-id=week-6 data-completed=false><div class=dot title=Planned></div><div class=content><div class=header><div class=left><div class=week>Week 6</div><div class=title><img class=icon src=/media/icons/timeline/attention.svg alt="attention icon">
<span>Flash-style Attention & KV Paged Cache (part 2)</span></div></div><div class=right></div></div><div class=summary>Redesign KV cache to paged/slab layout and add CUDA Graphs.</div><button class=toggle aria-expanded=false>Details</button><div class=details hidden><p>Introduce a paged (slab) KV cache to reduce fragmentation and enable large batches (vLLM-like PagedAttention semantics). Add CUDA Graph capture for decode and multiple CUDA streams to overlap prefill vs copies. Implement a simple scheduler to interleave requests by stage.</p><div class=comments><div class=comments-title>Notes</div><ul><li>M3: 2–3× throughput vs M2 at batch ≥ 8</li></ul></div></div></div></div><div class=timeline-item data-id=week-7 data-completed=false><div class=dot title=Planned></div><div class=content><div class=header><div class=left><div class=week>Week 7</div><div class=title><img class=icon src=/media/icons/timeline/transformer.svg alt="transformer icon">
<span>Quantization (AWQ path)</span></div></div><div class=right></div></div><div class=summary>Add weight-only INT4 via AWQ; validate accuracy vs fp16.</div><button class=toggle aria-expanded=false>Details</button><div class=details hidden><p>Implement AWQ calibration/scales with optional outlier handling. Validate perplexity drift and token accuracy compared to fp16/bf16 baselines. Prepare hooks for later GPTQ path.</p></div></div></div><div class=timeline-item data-id=week-8 data-completed=false><div class=dot title=Planned></div><div class=content><div class=header><div class=left><div class=week>Week 8</div><div class=title><img class=icon src=/media/icons/timeline/transformer.svg alt="transformer icon">
<span>Quantization (GPTQ/NF4) and comparisons</span></div></div><div class=right></div></div><div class=summary>Add GPTQ path; optional NF4 baseline; compare accuracy/speed.</div><button class=toggle aria-expanded=false>Details</button><div class=details hidden><p>Implement GPTQ group-wise quantization, and optionally NF4 (bitsandbytes-like) as a 4-bit baseline for comparison. Measure latency/bandwidth improvements and accuracy regressions.</p><div class=comments><div class=comments-title>Notes</div><ul><li>M4: INT4 path &lt; 0.5 ppl regression on WikiText-2</li></ul></div></div></div></div><div class=timeline-item data-id=week-9 data-completed=false><div class=dot title=Planned></div><div class=content><div class=header><div class=left><div class=week>Week 9</div><div class=title><img class=icon src=/media/icons/timeline/transformer.svg alt="transformer icon">
<span>Throughput Scalers</span></div></div><div class=right></div></div><div class=summary>In-flight batching, pinned I/O, overlap tokenization/H2D/decode.</div><button class=toggle aria-expanded=false>Details</button><div class=details hidden><p>Support joining requests mid-sequence. Use pinned host buffers and staging areas. Overlap tokenization, host-to-device copies, and decode steps for better utilization.</p><div class=comments><div class=comments-title>Notes</div><ul><li>Optional: tensor parallel across 2–4 GPUs (shard QKV/MLP)</li></ul></div></div></div></div><div class=timeline-item data-id=week-10 data-completed=false><div class=dot title=Planned></div><div class=content><div class=header><div class=left><div class=week>Week 10</div><div class=title><img class=icon src=/media/icons/timeline/transformer.svg alt="transformer icon">
<span>Speculative & Parallel Decoding</span></div></div><div class=right></div></div><div class=summary>Add Lookahead decoding; experiment with Medusa/ReDrafter.</div><button class=toggle aria-expanded=false>Details</button><div class=details hidden><p>Implement a flag to enable Lookahead decoding (parallel steps without a draft model). Add experimental adapters for Medusa (multi-head next tokens) or Apple’s ReDrafter. Keep interfaces pluggable for A/B testing.</p><div class=comments><div class=comments-title>Notes</div><ul><li>M5: 1.3–1.8× wall-clock speedup on long outputs</li></ul></div></div></div></div><div class=timeline-item data-id=week-11 data-completed=false><div class=dot title=Planned></div><div class=content><div class=header><div class=left><div class=week>Week 11</div><div class=title><img class=icon src=/media/icons/timeline/transformer.svg alt="transformer icon">
<span>Production Serving</span></div></div><div class=right></div></div><div class=summary>OpenAI-compatible REST, metrics, eviction policy, and hot-reload.</div><button class=toggle aria-expanded=false>Details</button><div class=details hidden><p>Implement /v1/chat/completions, rate limiting, logging, and Prometheus metrics. Add KV cache eviction (LRU by session) and guardrails for context window. Support tokenizer/weights hot-reload without downtime.</p></div></div></div><div class=timeline-item data-id=week-12 data-completed=false><div class=dot title=Planned></div><div class=content><div class=header><div class=left><div class=week>Week 12</div><div class=title><img class=icon src=/media/icons/timeline/transformer.svg alt="transformer icon">
<span>Docs, Benchmarks, Release</span></div></div><div class=right></div></div><div class=summary>Bench suite, competitive baselines, and public release.</div><button class=toggle aria-expanded=false>Details</button><div class=details hidden><p>Build a benchmark suite reporting tokens/s, p50/p95 latency, and VRAM usage vs HF baseline and llama.cpp on the same GPU. Reproduce vLLM/TensorRT-LLM ballpark throughput on your hardware and document where you gain/lose. Finalize docs and publish.</p><div class=comments><div class=comments-title>Notes</div><ul><li>M6: OpenAI-compatible API + metrics + report</li></ul></div></div></div></div></div></section><script defer src=/js/attention.js></script><link rel=stylesheet href=/css/attention.css></article></div><div class="flex flex-col items-center"><div class="container max-w-[65ch] mx-auto bg-white dark:bg-zinc-900 rounded-xl border-gray-100 dark:border-gray-700 border shadow-md overflow-hidden my-5"></div></div></div><div class=page-footer><footer class="container mx-auto flex flex-col justify-items-center text-sm leading-6 mt-24 mb-4 text-slate-700 dark:text-slate-200"><p class="powered-by text-center">© 2025 Me. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class="powered-by text-center">Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target=_blank rel=noopener>Hugo Blox Builder</a> — the free, <a href=https://github.com/HugoBlox/hugo-blox-builder target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></body></html>