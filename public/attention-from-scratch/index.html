<!doctype html>
<!-- This site was created with Hugo Blox. https://hugoblox.com -->
<!-- Last Published: October 18, 2025 --><html lang="en-us" dir="ltr"
      data-wc-theme-default="system">
  
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="generator" content="Hugo Blox Builder 0.3.1" />

  
  












  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Adin Mauer" />

  
  
  
    
  
  <meta name="description" content="Hardware/RTL engineer focused on high-performance interconnects, parallel computing, and applied ML. Projects, posts, and notes." />

  
  <link rel="alternate" hreflang="en-us" href="http://localhost:1313/attention-from-scratch/" />

  
  
  
  
    
    <link rel="stylesheet" href="/css/themes/emerald.min.css" />
  

  
  
    
    <link href="/dist/wc.min.css" rel="stylesheet" />
  

  
  
  

  
    
    <link href="/css/custom.min.c85eb71a15c00cd07612e42026afad4b490b997f687f5cca8c13cb273edc5c1b.css" rel="stylesheet" />
  

  <script>
     
    window.hbb = {
       defaultTheme: document.documentElement.dataset.wcThemeDefault,
       setDarkTheme: () => {
        document.documentElement.classList.add("dark");
        document.documentElement.style.colorScheme = "dark";
      },
       setLightTheme: () => {
        document.documentElement.classList.remove("dark");
        document.documentElement.style.colorScheme = "light";
      }
    }

    console.debug(`Default Hugo Blox Builder theme is ${window.hbb.defaultTheme}`);

    if ("wc-color-theme" in localStorage) {
      localStorage.getItem("wc-color-theme") === "dark" ? window.hbb.setDarkTheme() : window.hbb.setLightTheme();
    } else {
      window.hbb.defaultTheme === "dark" ? window.hbb.setDarkTheme() : window.hbb.setLightTheme();
      if (window.hbb.defaultTheme === "system") {
        window.matchMedia("(prefers-color-scheme: dark)").matches ? window.hbb.setDarkTheme() : window.hbb.setLightTheme();
      }
    }
  </script>

  <script>
    
    document.addEventListener('DOMContentLoaded', function () {
      
      let checkboxes = document.querySelectorAll('li input[type=\'checkbox\'][disabled]');
      checkboxes.forEach(e => {
        e.parentElement.parentElement.classList.add('task-list');
      });

      
      const liNodes = document.querySelectorAll('.task-list li');
      liNodes.forEach(nodes => {
        let textNodes = Array.from(nodes.childNodes).filter(node => node.nodeType === 3 && node.textContent.trim().length > 1);
        if (textNodes.length > 0) {
          const span = document.createElement('label');
          textNodes[0].after(span);  
          span.appendChild(nodes.querySelector('input[type=\'checkbox\']'));
          span.appendChild(textNodes[0]);
        }
      });
    });
  </script>

  
  
  




































  
  

  
  <link rel="icon" type="image/png" href="/media/icon_hu_c677720d886dbb6f.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu_67b40f4ae84a6376.png" />

  <link rel="canonical" href="http://localhost:1313/attention-from-scratch/" />

  
  
  
  
  
  
  
  
    
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="Adin Mauer Portfolio Website" />
  <meta property="og:url" content="http://localhost:1313/attention-from-scratch/" />
  <meta property="og:title" content="Attention From Scratch | Adin Mauer Portfolio Website" />
  <meta property="og:description" content="Hardware/RTL engineer focused on high-performance interconnects, parallel computing, and applied ML. Projects, posts, and notes." /><meta property="og:image" content="http://localhost:1313/media/icon_hu_95c3e26984a71978.png" />
    <meta property="twitter:image" content="http://localhost:1313/media/icon_hu_95c3e26984a71978.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta property="og:updated_time" content="2025-01-01T00:00:00&#43;00:00" />
    
  

  




  <title>Attention From Scratch | Adin Mauer Portfolio Website</title>

  
  
  
  
  
    
    
  
  
  <style>
    @font-face {
      font-family: 'Inter var';
      font-style: normal;
      font-weight: 100 900;
      font-display: swap;
      src: url(/dist/font/Inter.var.woff2) format(woff2);
    }
  </style>

  

  
  


  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  
  
  
  
  
  
  
  
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  
  
  
  
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
    
  
  
  
  
  
  
  
  
  
  
  
  














  
  
  <link type="text/css" rel="stylesheet" href="/dist/lib/katex/katex.min.505d5f829022bb7b4f24dfee0aa1141cd7bba67afe411d1240335f820960b5c3.css" integrity="sha256-UF1fgpAiu3tPJN/uCqEUHNe7pnr&#43;QR0SQDNfgglgtcM=" />
  
  
  <script defer src="/dist/lib/katex/katex.min.dc84b296ec3e884de093158f760fd9d45b6c7abe58b5381557f4e138f46a58ae.js" integrity="sha256-3ISyluw&#43;iE3gkxWPdg/Z1Ftser5YtTgVV/ThOPRqWK4="></script>
  
  
  
  
  <script defer src="/js/katex-renderer.6579ec9683211cfb952064aedf3a3baea5eeb17a061775b32b70917474637c80.js" integrity="sha256-ZXnsloMhHPuVIGSu3zo7rqXusXoGF3WzK3CRdHRjfIA="></script>
  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  






  
  
  
  
  
  
  
  <script
    defer
    src="/js/hugo-blox-en.min.js"
    integrity=""
  ></script>

  
  








  
    
      
      <script async defer src="https://buttons.github.io/buttons.js"></script>

      
    
  




</head>

  <body class="dark:bg-hb-dark dark:text-white page-wrapper" id="top">
    <div id="page-bg"></div>
    <div class="page-header sticky top-0 z-30">
      
      
      
        
        
        
          <header id="site-header" class="header">
  <nav class="navbar px-3 flex justify-left">
    <div class="order-0 h-100">
      
      <a class="navbar-brand" href="/" title="Adin Mauer Portfolio Website">
        Adin Mauer
      </a>
    </div>
    
    <input id="nav-toggle" type="checkbox" class="hidden" />
    <label
      for="nav-toggle"
      class="order-3 cursor-pointer flex items-center lg:hidden text-dark dark:text-white lg:order-1">
      <svg id="show-button" class="h-6 fill-current block" viewBox="0 0 20 20">
        <title>Open Menu</title>
        <path d="M0 3h20v2H0V3z m0 6h20v2H0V9z m0 6h20v2H0V0z"></path>
      </svg>
      <svg id="hide-button" class="h-6 fill-current hidden" viewBox="0 0 20 20">
        <title>Close Menu</title>
        <polygon
          points="11 9 22 9 22 11 11 11 11 22 9 22 9 11 -2 11 -2 9 9 9 9 -2 11 -2"
          transform="rotate(45 10 10)"></polygon>
      </svg>
    </label>
    

    
    
    <ul
      id="nav-menu"
      class="navbar-nav order-3 hidden lg:flex w-full pb-6 lg:order-1 lg:w-auto lg:space-x-2 lg:pb-0 xl:space-x-8 justify-left
      ">
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/"
        >Bio</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/experience/"
        >Experience</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/projects/"
        >Projects</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/post/"
        >Blog</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link active"
          
          href="/attention-from-scratch/"
        >Attention From Scratch</a
        >
      </li>
      
      
      
    </ul>

    <div class="order-1 ml-auto flex items-center md:order-2 lg:ml-0">

      
      
      

      
      
      <div class="px-3 text-black hover:text-primary-700 dark:text-white dark:hover:text-primary-300
            [&.active]:font-bold [&.active]:text-black/90 dark:[&.active]:text-white">
        <button class="theme-toggle mt-1" accesskey="t" title="appearance">
          <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
               fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
               stroke-linejoin="round" class="dark:hidden">
            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
          </svg>
          <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
               fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
               stroke-linejoin="round" class=" dark:block [&:not(dark)]:hidden">
            <circle cx="12" cy="12" r="5"></circle>
            <line x1="12" y1="1" x2="12" y2="3"></line>
            <line x1="12" y1="21" x2="12" y2="23"></line>
            <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
            <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
            <line x1="1" y1="12" x2="3" y2="12"></line>
            <line x1="21" y1="12" x2="23" y2="12"></line>
            <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
            <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
          </svg>
        </button>
      </div>
      

      
      

      
      
    </div>
  </nav>
</header>



        
      
    </div>
    <div class="page-body  my-10">
      

<div class="max-w-prose mx-auto flex justify-center">
  <article class="prose prose-slate lg:prose-xl dark:prose-invert">
    <h1 class="lg:text-6xl">Attention From Scratch</h1>
    <p>I&rsquo;m mapping the landscape of large language models to see what is possible for an individual, not just massive tech teams. The exciting news is that fully open-source LLMs exist: you can inspect the weights, study the architecture, run the inference code, and even examine the training stack. As an engineer who loves blending hardware, software, and math &ndash; classic HW/SW co-design &ndash; I&rsquo;m inspired by the breakthroughs that have pushed LLM performance forward: FlashAttention and its fused kernels, asynchronous attention in FlashAttention 2 and 3, and paged-attention serving systems like vLLM. With today&rsquo;s cloud options, renting enterprise-class GPUs for inference is also within reach. So I&rsquo;m charting a personal roadmap from beginner to running production-grade inference infrastructure on top of open models like Olmo 2. I&rsquo;m not trying to train new models; I&rsquo;m focused on running them exceptionally well. Along the way I hope to understand the algorithms inside one of the most impactful technologies of our time, and maybe even find new ways to push them further.</p>


<section class="attention-hero">
  <div class="container">
    <h1>Attention From Scratch</h1>
    <p class="subtitle">12-week plan to build a from-scratch Transformer inference stack (prefill, decode, serving)</p>
    
    <div class="meta-bar" role="group" aria-label="Plan dates">
      <div class="meta-chip">
        <span class="meta-emoji">ðŸ“†</span>
        <span class="meta-label">Today</span>
        <span class="meta-value">2025-10-18</span>
      </div>
      <div class="meta-chip">
        <span class="meta-emoji">ðŸš€</span>
        <span class="meta-label">Start</span>
        <span class="meta-value">2025-10-14</span>
      </div>
      <div class="meta-chip">
        <span class="meta-emoji">ðŸ”„</span>
        <span class="meta-label">Updated</span>
        <span class="meta-value">2030-06-01</span>
      </div>
    </div>
    
    
    
    
    <div class="progress">
      <div class="progress-label">Progress: 0/12 (0%)</div>
      <div class="progress-outer" aria-label="Plan progress" role="progressbar" aria-valuemin="0" aria-valuemax="100" aria-valuenow="0">
        <div class="progress-inner" style="width: 0%"></div>
      </div>
    </div>
  </div>
</section>

<section class="attention-timeline">
  <div class="timeline-wrapper">
    <div class="timeline-list">
      
      <div class="timeline-item" data-id="week-1" data-week="1" data-title="Vast.AI Setup &#43; Baseline Environment" data-target="2025-10-21" data-completed="false">
        <div class="dot " title="Planned"></div>
        <div class="content" tabindex="0" role="button" aria-label="Open details for week 1" title="Click for details and checklist (Enter to open)">
          <div class="header">
            <div class="left">
              <div class="week">Week 1</div>
              <div class="title"><span>Vast.AI Setup &#43; Baseline Environment</span></div>
            </div>
            <div class="right">
              <div class="target">Target: 2025-10-21</div>
              
            </div>
          </div>
          <div class="summary">Provision a GPU instance, set up toolchains and libraries, and capture a baseline with a reference LLM.</div>
          <div class="cta-row" aria-hidden="true"><span class="cta-text">View details & checklist</span><span class="cta-chev">â†—</span></div>
          <div class="overlay-template" hidden>
            <div class="overlay-header">
              <div class="overlay-week">Week 1</div>
              <div class="overlay-title">Vast.AI Setup &#43; Baseline Environment</div>
              <div class="overlay-target">Target: 2025-10-21</div>
            </div>
            <div class="overlay-body">
              <p>Create a repeatable Vast.AI environment. Install build tools, CUDA/CUTLASS, SentencePiece, and HF libs. Run a reference model (Mistral-7B or Llama-3-8B) to capture tokens/s, latency, VRAM usage â€” your baseline.</p>
              
              <div class="overlay-checklist">
                <div class="checklist-title">Checklist</div>
                <ul>
                  
                    <li><input type="checkbox" disabled /> <span>Create Vast.AI account and allocate A100/4090 GPU</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Create a persistent Volume for models and build cache</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>map[Launch image:nvcr.io/nvidia/pytorch:25.04-py3]</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Install build tools (git, cmake, ninja, g&#43;&#43;, clang, python3-dev)</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Install libs (torch, transformers, safetensors, sentencepiece, numpy, cutlass, pybind11)</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Verify CUDA/NVCC and run a sample kernel</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>map[Run reference LLM and log:tokens/s, VRAM, prefill vs decode latency]</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Save results as baseline report</span></li>
                  
                </ul>
              </div>
              
              
            </div>
          </div>
        </div>
      </div>
      
      <div class="timeline-item" data-id="week-2" data-week="2" data-title="Repo Scaffolding &#43; Tokenizer &#43; Weight Loader" data-target="2025-10-28" data-completed="false">
        <div class="dot " title="Planned"></div>
        <div class="content" tabindex="0" role="button" aria-label="Open details for week 2" title="Click for details and checklist (Enter to open)">
          <div class="header">
            <div class="left">
              <div class="week">Week 2</div>
              <div class="title"><span>Repo Scaffolding &#43; Tokenizer &#43; Weight Loader</span></div>
            </div>
            <div class="right">
              <div class="target">Target: 2025-10-28</div>
              
            </div>
          </div>
          <div class="summary">Scaffold the project and implement tokenizer wrapper and safetensors weight loader with tests.</div>
          <div class="cta-row" aria-hidden="true"><span class="cta-text">View details & checklist</span><span class="cta-chev">â†—</span></div>
          <div class="overlay-template" hidden>
            <div class="overlay-header">
              <div class="overlay-week">Week 2</div>
              <div class="overlay-title">Repo Scaffolding &#43; Tokenizer &#43; Weight Loader</div>
              <div class="overlay-target">Target: 2025-10-28</div>
            </div>
            <div class="overlay-body">
              <p>Create C&#43;&#43;/CUDA skeleton with CMake and pybind11 bindings. Implement SentencePiece wrapper and a safetensors loader with shape/name validation. Add unit tests with small diffs.</p>
              
              <div class="overlay-checklist">
                <div class="checklist-title">Checklist</div>
                <ul>
                  
                    <li><input type="checkbox" disabled /> <span>Init src/, include/, python_bindings/, tests/, scripts/, benchmarks/</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>CMakeLists for shared lib and exe targets</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>SentencePiece tokenizer wrapper (&#43; round-trip tests)</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Safetensors reader to row-major fp16; validate names/shapes</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>map[Unit tests:mean abs diff &lt; 1e-5 on sampled tensors]</span></li>
                  
                </ul>
              </div>
              
              
            </div>
          </div>
        </div>
      </div>
      
      <div class="timeline-item" data-id="week-3" data-week="3" data-title="Forward Pass Framework &#43; Basic Kernels" data-target="2025-11-04" data-completed="false">
        <div class="dot " title="Planned"></div>
        <div class="content" tabindex="0" role="button" aria-label="Open details for week 3" title="Click for details and checklist (Enter to open)">
          <div class="header">
            <div class="left">
              <div class="week">Week 3</div>
              <div class="title"><span>Forward Pass Framework &#43; Basic Kernels</span></div>
            </div>
            <div class="right">
              <div class="target">Target: 2025-11-04</div>
              
            </div>
          </div>
          <div class="summary">Build the forward graph and write core kernels for RMSNorm, RoPE, matmul, MLP, and naÃ¯ve attention.</div>
          <div class="cta-row" aria-hidden="true"><span class="cta-text">View details & checklist</span><span class="cta-chev">â†—</span></div>
          <div class="overlay-template" hidden>
            <div class="overlay-header">
              <div class="overlay-week">Week 3</div>
              <div class="overlay-title">Forward Pass Framework &#43; Basic Kernels</div>
              <div class="overlay-target">Target: 2025-11-04</div>
            </div>
            <div class="overlay-body">
              <p>Implement RMSNorm and RoPE kernels. Use cuBLAS/CUTLASS for matmul wrappers. Implement SwiGLU MLP and a simple attention path. Compare layerwise activations against HF.</p>
              
              <div class="overlay-checklist">
                <div class="checklist-title">Checklist</div>
                <ul>
                  
                    <li><input type="checkbox" disabled /> <span>Define ModelConfig (layers, heads, dims, rope_theta)</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>RMSNorm kernel validated vs PyTorch</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>RoPE kernel</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Matmul wrapper (cuBLAS/CUTLASS)</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>MLP (SwiGLU)</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>NaÃ¯ve QKV attention &#43; softmax</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Prefill-only forward; layerwise MAE &lt; 1e-3</span></li>
                  
                </ul>
              </div>
              
              
            </div>
          </div>
        </div>
      </div>
      
      <div class="timeline-item" data-id="week-4" data-week="4" data-title="Prefill &#43; Decode Loop &#43; Sampling" data-target="2025-11-11" data-completed="false">
        <div class="dot " title="Planned"></div>
        <div class="content" tabindex="0" role="button" aria-label="Open details for week 4" title="Click for details and checklist (Enter to open)">
          <div class="header">
            <div class="left">
              <div class="week">Week 4</div>
              <div class="title"><span>Prefill &#43; Decode Loop &#43; Sampling</span></div>
            </div>
            <div class="right">
              <div class="target">Target: 2025-11-11</div>
              
            </div>
          </div>
          <div class="summary">Implement KV cache, decode loop, and sampling strategies; validate generations and measure performance.</div>
          <div class="cta-row" aria-hidden="true"><span class="cta-text">View details & checklist</span><span class="cta-chev">â†—</span></div>
          <div class="overlay-template" hidden>
            <div class="overlay-header">
              <div class="overlay-week">Week 4</div>
              <div class="overlay-title">Prefill &#43; Decode Loop &#43; Sampling</div>
              <div class="overlay-target">Target: 2025-11-11</div>
            </div>
            <div class="overlay-body">
              <p>Add contiguous KV cache, greedy/top-k/top-p/temperature sampling, and end-to-end decode. Measure throughput and latency; validate against HF on short prompts.</p>
              
              <div class="overlay-checklist">
                <div class="checklist-title">Checklist</div>
                <ul>
                  
                    <li><input type="checkbox" disabled /> <span>KV cache structure (batch, layer, seq, dim)</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Prefill &#43; token-by-token decode loop</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Greedy, top-k, top-p, temperature</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Validate text parity on short prompts</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Record tokens/s, latency, memory use</span></li>
                  
                </ul>
              </div>
              
              
            </div>
          </div>
        </div>
      </div>
      
      <div class="timeline-item" data-id="week-5" data-week="5" data-title="FlashAttention-Style Optimization" data-target="2025-11-18" data-completed="false">
        <div class="dot " title="Planned"></div>
        <div class="content" tabindex="0" role="button" aria-label="Open details for week 5" title="Click for details and checklist (Enter to open)">
          <div class="header">
            <div class="left">
              <div class="week">Week 5</div>
              <div class="title"><span>FlashAttention-Style Optimization</span></div>
            </div>
            <div class="right">
              <div class="target">Target: 2025-11-18</div>
              
            </div>
          </div>
          <div class="summary">Replace naÃ¯ve attention with IO-aware tiled kernels; benchmark and profile.</div>
          <div class="cta-row" aria-hidden="true"><span class="cta-text">View details & checklist</span><span class="cta-chev">â†—</span></div>
          <div class="overlay-template" hidden>
            <div class="overlay-header">
              <div class="overlay-week">Week 5</div>
              <div class="overlay-title">FlashAttention-Style Optimization</div>
              <div class="overlay-target">Target: 2025-11-18</div>
            </div>
            <div class="overlay-body">
              <p>Implement shared-memory tiled softmax/attention; test on 128â€“4096 tokens. Benchmark vs week-4 and capture Nsight metrics (occupancy, memory bandwidth).</p>
              
              <div class="overlay-checklist">
                <div class="checklist-title">Checklist</div>
                <ul>
                  
                    <li><input type="checkbox" disabled /> <span>Review FlashAttention algorithm</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Tiled softmax&#43;attention kernel</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Correctness on 128â€“4096 tokens</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Benchmark vs week-4 attention</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Nsight Compute profiling and timing macros</span></li>
                  
                </ul>
              </div>
              
              
            </div>
          </div>
        </div>
      </div>
      
      <div class="timeline-item" data-id="week-6" data-week="6" data-title="Advanced KV Cache &#43; CUDA Graphs" data-target="2025-11-25" data-completed="false">
        <div class="dot " title="Planned"></div>
        <div class="content" tabindex="0" role="button" aria-label="Open details for week 6" title="Click for details and checklist (Enter to open)">
          <div class="header">
            <div class="left">
              <div class="week">Week 6</div>
              <div class="title"><span>Advanced KV Cache &#43; CUDA Graphs</span></div>
            </div>
            <div class="right">
              <div class="target">Target: 2025-11-25</div>
              
            </div>
          </div>
          <div class="summary">Introduce paged/slab KV cache and capture the decode path with CUDA Graphs.</div>
          <div class="cta-row" aria-hidden="true"><span class="cta-text">View details & checklist</span><span class="cta-chev">â†—</span></div>
          <div class="overlay-template" hidden>
            <div class="overlay-header">
              <div class="overlay-week">Week 6</div>
              <div class="overlay-title">Advanced KV Cache &#43; CUDA Graphs</div>
              <div class="overlay-target">Target: 2025-11-25</div>
            </div>
            <div class="overlay-body">
              <p>Design a paged allocator (vLLM-style) handling variable seq lengths and in-flight batching. Add CUDA Graphs for the decode chain and micro-bench scheduling overhead.</p>
              
              <div class="overlay-checklist">
                <div class="checklist-title">Checklist</div>
                <ul>
                  
                    <li><input type="checkbox" disabled /> <span>Paged/slab KV allocator (vLLM-inspired)</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Variable seq lengths &#43; in-flight batching</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>CUDA Graph capture for decode chain</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Micro-bench capture overhead; target â‰¤ 1 Âµs per token</span></li>
                  
                </ul>
              </div>
              
              
            </div>
          </div>
        </div>
      </div>
      
      <div class="timeline-item" data-id="week-7" data-week="7" data-title="Quantization (AWQ / GPTQ / NF4)" data-target="2025-12-02" data-completed="false">
        <div class="dot " title="Planned"></div>
        <div class="content" tabindex="0" role="button" aria-label="Open details for week 7" title="Click for details and checklist (Enter to open)">
          <div class="header">
            <div class="left">
              <div class="week">Week 7</div>
              <div class="title"><span>Quantization (AWQ / GPTQ / NF4)</span></div>
            </div>
            <div class="right">
              <div class="target">Target: 2025-12-02</div>
              
            </div>
          </div>
          <div class="summary">Implement weight-only int4 paths and evaluate quality vs fp16 baseline.</div>
          <div class="cta-row" aria-hidden="true"><span class="cta-text">View details & checklist</span><span class="cta-chev">â†—</span></div>
          <div class="overlay-template" hidden>
            <div class="overlay-header">
              <div class="overlay-week">Week 7</div>
              <div class="overlay-title">Quantization (AWQ / GPTQ / NF4)</div>
              <div class="overlay-target">Target: 2025-12-02</div>
            </div>
            <div class="overlay-body">
              <p>Implement group-wise quantization with on-the-fly dequant in GEMM, compare AWQ/GPTQ/NF4 behavior and regressions.</p>
              
              <div class="overlay-checklist">
                <div class="checklist-title">Checklist</div>
                <ul>
                  
                    <li><input type="checkbox" disabled /> <span>Group-wise quantization (e.g., 128-group)</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Scales/zeros and storage format</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>GEMM dequant on-the-fly</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Logit diff within tolerance vs fp16</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Throughput &amp; memory evaluation</span></li>
                  
                </ul>
              </div>
              
              
            </div>
          </div>
        </div>
      </div>
      
      <div class="timeline-item" data-id="week-8" data-week="8" data-title="Batching, Streams, and Scheduler" data-target="2025-12-09" data-completed="false">
        <div class="dot " title="Planned"></div>
        <div class="content" tabindex="0" role="button" aria-label="Open details for week 8" title="Click for details and checklist (Enter to open)">
          <div class="header">
            <div class="left">
              <div class="week">Week 8</div>
              <div class="title"><span>Batching, Streams, and Scheduler</span></div>
            </div>
            <div class="right">
              <div class="target">Target: 2025-12-09</div>
              
            </div>
          </div>
          <div class="summary">Improve throughput via concurrent streams and request scheduling.</div>
          <div class="cta-row" aria-hidden="true"><span class="cta-text">View details & checklist</span><span class="cta-chev">â†—</span></div>
          <div class="overlay-template" hidden>
            <div class="overlay-header">
              <div class="overlay-week">Week 8</div>
              <div class="overlay-title">Batching, Streams, and Scheduler</div>
              <div class="overlay-target">Target: 2025-12-09</div>
            </div>
            <div class="overlay-body">
              <p>Build prefill/decode queues with separate CUDA streams, overlap tokenization and H2D copies, and benchmark mixed prompt lengths.</p>
              
              <div class="overlay-checklist">
                <div class="checklist-title">Checklist</div>
                <ul>
                  
                    <li><input type="checkbox" disabled /> <span>Request queues (prefill/decode)</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Separate CUDA streams for prefill/copy</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Overlap tokenization and H2D</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>â‰¥2Ã— tokens/s vs week-6 with batching</span></li>
                  
                </ul>
              </div>
              
              
            </div>
          </div>
        </div>
      </div>
      
      <div class="timeline-item" data-id="week-9" data-week="9" data-title="Parallelism &amp; Memory Optimization" data-target="2025-12-16" data-completed="false">
        <div class="dot " title="Planned"></div>
        <div class="content" tabindex="0" role="button" aria-label="Open details for week 9" title="Click for details and checklist (Enter to open)">
          <div class="header">
            <div class="left">
              <div class="week">Week 9</div>
              <div class="title"><span>Parallelism &amp; Memory Optimization</span></div>
            </div>
            <div class="right">
              <div class="target">Target: 2025-12-16</div>
              
            </div>
          </div>
          <div class="summary">Extend to multi-GPU tensor parallel and reduce memory pressure for long contexts.</div>
          <div class="cta-row" aria-hidden="true"><span class="cta-text">View details & checklist</span><span class="cta-chev">â†—</span></div>
          <div class="overlay-template" hidden>
            <div class="overlay-header">
              <div class="overlay-week">Week 9</div>
              <div class="overlay-title">Parallelism &amp; Memory Optimization</div>
              <div class="overlay-target">Target: 2025-12-16</div>
            </div>
            <div class="overlay-body">
              <p>Shard QKV/MLP across GPUs with NCCL all-reduce; add activation checkpointing for long contexts and measure scaling.</p>
              
              <div class="overlay-checklist">
                <div class="checklist-title">Checklist</div>
                <ul>
                  
                    <li><input type="checkbox" disabled /> <span>Tensor parallel sharding for QKV/MLP</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>NCCL all-reduce wiring and tests</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>1â†’2 GPU scaling measurements</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Activation checkpointing for long contexts</span></li>
                  
                </ul>
              </div>
              
              
            </div>
          </div>
        </div>
      </div>
      
      <div class="timeline-item" data-id="week-10" data-week="10" data-title="Speculative &amp; Lookahead Decoding" data-target="2025-12-23" data-completed="false">
        <div class="dot " title="Planned"></div>
        <div class="content" tabindex="0" role="button" aria-label="Open details for week 10" title="Click for details and checklist (Enter to open)">
          <div class="header">
            <div class="left">
              <div class="week">Week 10</div>
              <div class="title"><span>Speculative &amp; Lookahead Decoding</span></div>
            </div>
            <div class="right">
              <div class="target">Target: 2025-12-23</div>
              
            </div>
          </div>
          <div class="summary">Implement Lookahead and (optionally) Medusa heads; measure speedups.</div>
          <div class="cta-row" aria-hidden="true"><span class="cta-text">View details & checklist</span><span class="cta-chev">â†—</span></div>
          <div class="overlay-template" hidden>
            <div class="overlay-header">
              <div class="overlay-week">Week 10</div>
              <div class="overlay-title">Speculative &amp; Lookahead Decoding</div>
              <div class="overlay-target">Target: 2025-12-23</div>
            </div>
            <div class="overlay-body">
              <p>Enable a lookahead path with rollback, add an optional Medusa multi-token predictor, and log speedup ratios on long outputs.</p>
              
              <div class="overlay-checklist">
                <div class="checklist-title">Checklist</div>
                <ul>
                  
                    <li><input type="checkbox" disabled /> <span>Lookahead decoding with rollback checks</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Optional Medusa head adapter</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Measure speedup vs standard decode</span></li>
                  
                </ul>
              </div>
              
              
            </div>
          </div>
        </div>
      </div>
      
      <div class="timeline-item" data-id="week-11" data-week="11" data-title="Serving Layer" data-target="2025-12-30" data-completed="false">
        <div class="dot " title="Planned"></div>
        <div class="content" tabindex="0" role="button" aria-label="Open details for week 11" title="Click for details and checklist (Enter to open)">
          <div class="header">
            <div class="left">
              <div class="week">Week 11</div>
              <div class="title"><span>Serving Layer</span></div>
            </div>
            <div class="right">
              <div class="target">Target: 2025-12-30</div>
              
            </div>
          </div>
          <div class="summary">Expose the engine via a lightweight API with batching and metrics.</div>
          <div class="cta-row" aria-hidden="true"><span class="cta-text">View details & checklist</span><span class="cta-chev">â†—</span></div>
          <div class="overlay-template" hidden>
            <div class="overlay-header">
              <div class="overlay-week">Week 11</div>
              <div class="overlay-title">Serving Layer</div>
              <div class="overlay-target">Target: 2025-12-30</div>
            </div>
            <div class="overlay-body">
              <p>Implement HTTP endpoints (/v1/completions and /v1/chat/completions), add batching inside the server, metrics, and KV eviction.</p>
              
              <div class="overlay-checklist">
                <div class="checklist-title">Checklist</div>
                <ul>
                  
                    <li><input type="checkbox" disabled /> <span>FastAPI or C&#43;&#43; http server</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>/v1/completions and /v1/chat/completions</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>In-server batching</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Metrics and LRU cache eviction</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Concurrency test with â‰¥ 8 clients</span></li>
                  
                </ul>
              </div>
              
              
            </div>
          </div>
        </div>
      </div>
      
      <div class="timeline-item" data-id="week-12" data-week="12" data-title="Validation, Docs, and Release" data-target="2026-01-06" data-completed="false">
        <div class="dot " title="Planned"></div>
        <div class="content" tabindex="0" role="button" aria-label="Open details for week 12" title="Click for details and checklist (Enter to open)">
          <div class="header">
            <div class="left">
              <div class="week">Week 12</div>
              <div class="title"><span>Validation, Docs, and Release</span></div>
            </div>
            <div class="right">
              <div class="target">Target: 2026-01-06</div>
              
            </div>
          </div>
          <div class="summary">Validate parity and publish a complete report and repo.</div>
          <div class="cta-row" aria-hidden="true"><span class="cta-text">View details & checklist</span><span class="cta-chev">â†—</span></div>
          <div class="overlay-template" hidden>
            <div class="overlay-header">
              <div class="overlay-week">Week 12</div>
              <div class="overlay-title">Validation, Docs, and Release</div>
              <div class="overlay-target">Target: 2026-01-06</div>
            </div>
            <div class="overlay-body">
              <p>Run numerical parity checks, benchmark against llama.cpp/vLLM/TensorRT-LLM, stress test long prompts, and publish a thorough write-up with results and lessons learned.</p>
              
              <div class="overlay-checklist">
                <div class="checklist-title">Checklist</div>
                <ul>
                  
                    <li><input type="checkbox" disabled /> <span>Logit diff &lt; 1e-3 vs HF</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Bench vs llama.cpp, vLLM, TensorRT-LLM</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Long-prompt stress tests (&gt; 8k tokens)</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Technical report &#43; diagrams &#43; profiling</span></li>
                  
                    <li><input type="checkbox" disabled /> <span>Public repo and (optional) demo video</span></li>
                  
                </ul>
              </div>
              
              
            </div>
          </div>
        </div>
      </div>
      
    </div>
  </div>
</section>

<div class="attention-overlay" hidden aria-hidden="true">
  <div class="overlay-backdrop"></div>
  <div class="overlay-panel" role="dialog" aria-modal="true" aria-label="Week details">
    <button class="overlay-close" aria-label="Close">âœ•</button>
    <div class="overlay-content">
      
    </div>
  </div>
</div>


<script defer src="/js/attention.min.df25e8453bc152bb0027415a0a47162e4fa47b7a71d06ab6d1386e98955a79f2.js"></script>


  </article>
</div>




<div class="flex flex-col items-center">

  








<a href="/attention-from-scratch/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Attention From Scratch</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        Iâ€™m mapping the landscape of large language models to see what is possible for an individual, not just massive tech teams. The exciting news is that fully open-source LLMs exist: you can inspect the weights, study the architecture, run the inference code, and even examine the training stack. As an engineer who loves blending hardware, software, and math â€“ classic HW/SW co-design â€“ Iâ€™m inspired by the breakthroughs that have pushed LLM performance forward: FlashAttention and its fused kernels, asynchronous attention in FlashAttention 2 and 3, and paged-attention serving systems like vLLM. With todayâ€™s cloud options, renting enterprise-class GPUs for inference is also within reach. So Iâ€™m charting a personal roadmap from beginner to running production-grade inference infrastructure on top of open models like Olmo 2. Iâ€™m not trying to train new models; Iâ€™m focused on running them exceptionally well. Along the way I hope to understand the algorithms inside one of the most impactful technologies of our time, and maybe even find new ways to push them further.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Jan 1, 2025</p>
    </div>
  </div>
</a>


  
  

  








<a href="/attention-from-scratch/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Attention From Scratch</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        Iâ€™m mapping the landscape of large language models to see what is possible for an individual, not just massive tech teams. The exciting news is that fully open-source LLMs exist: you can inspect the weights, study the architecture, run the inference code, and even examine the training stack. As an engineer who loves blending hardware, software, and math â€“ classic HW/SW co-design â€“ Iâ€™m inspired by the breakthroughs that have pushed LLM performance forward: FlashAttention and its fused kernels, asynchronous attention in FlashAttention 2 and 3, and paged-attention serving systems like vLLM. With todayâ€™s cloud options, renting enterprise-class GPUs for inference is also within reach. So Iâ€™m charting a personal roadmap from beginner to running production-grade inference infrastructure on top of open models like Olmo 2. Iâ€™m not trying to train new models; Iâ€™m focused on running them exceptionally well. Along the way I hope to understand the algorithms inside one of the most impactful technologies of our time, and maybe even find new ways to push them further.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Jan 1, 2025</p>
    </div>
  </div>
</a>


  


</div>


    </div>
    <div class="page-footer">
      <footer class="container mx-auto flex flex-col justify-items-center text-sm leading-6 mt-24 mb-4 text-slate-700 dark:text-slate-200">

  












  
  
  
  
  














  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by text-center">
    Â© 2025 Me. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>





  <p class="powered-by text-center">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Hugo Blox Builder</a> â€” the free, <a href="https://github.com/HugoBlox/hugo-blox-builder" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>

</footer>

    </div>

    
    











  </body>
</html>
